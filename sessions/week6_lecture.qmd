---
title: "Prof D's Regression Sessions - Vol 1"
subtitle: "Time to go deep!"
author: 
  - name: "Adam Dennett"
email: "a.dennett@ucl.ac.uk"
date-as-string: "1st August 2024"
other: "CASA0007 Quantitative Methods"
from: markdown+emoji
format:
  revealjs: 
    logo: "L6_images/CASA_logo.svg"
    template-partials: 
      - title-slide.html
    transition: none
    slide-number: TRUE
    preview-links: auto
    theme: casa-slides
    
filters:
 - code-visibility
lightbox: auto
title-slide-attributes:
    data-background-image: "L6_images/regression.png"
    data-background-size: stretch
    data-background-opacity: "0.08"
    data-background-color: "#4e3c56"
---

```{r}
#| label: load-data
#| message: false
#| warning: false
#| include: false

#casaviz::view_palette(casaviz::casa_palettes$default, n_colours = 10)

##before we do anything else, let's load the packages we need and the data we require

library(casaviz)
library(tidyverse)
library(sf)
library(plotly)
library(leaflet)
library(rgl)
library(dplyr)
library(here)
library(stringr)
library(dplyr)
library(purrr)
library(janitor)
library(readxl)
library(tibble)
library(ggrepel)
library(gganimate)
#library(easystats)

#all england schools
edubase_schools <- read_csv("https://www.dropbox.com/scl/fi/fhzafgt27v30lmmuo084y/edubasealldata20241003.csv?rlkey=uorw43s44hnw5k9js3z0ksuuq&raw=1") %>% 
  clean_names() %>% 
  mutate(urn = as.character(urn))

#all england schools
#edubase_schools <- read.csv("C:/Users/Adam/Dropbox/Public/edubasealldata20241003.csv") %>% 
#  clean_names() %>% 
#  mutate(urn = as.character(urn))

england_abs <- read_csv(here("sessions", "L6_data", "Performancetables_130242", "2022-2023", "england_abs.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>%
  mutate(URN = as.character(URN))
england_census <- read_csv(here("sessions", "L6_data", "Performancetables_130242", "2022-2023", "england_census.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>%
  mutate(URN = as.character(URN)) %>%
  mutate(across(5:23, ~ parse_number(as.character(.))))
england_ks4_mats_performance <- read_csv(
  here("sessions", "L6_data", "Performancetables_130242", "2022-2023", "england_ks4-mats-performance.csv"),
  na = c("", "NA", "SUPPMAT", "NP", "NE")
) %>%
  mutate(
    TRUST_UID = as.character(TRUST_UID),
    P8_BANDING = as.character(P8_BANDING),
    INSTITUTIONS_INMAT = as.character(INSTITUTIONS_INMAT)
  ) %>%
  mutate(across(
    .cols = names(.)[11:ncol(.)][!names(.)[11:ncol(.)] %in% c("P8_BANDING", "INSTITUTIONS_INMAT")],
    .fns = ~ parse_number(as.character(.))
  ))

england_ks4_pupdest <- read_csv(here("sessions", "L6_data", "Performancetables_130242", "2022-2023", "england_ks4-pupdest.csv"), na = c("", "NA", "SUPP", "NP", "NE", "SP", "SN")) %>%
  mutate(URN = as.character(URN)) %>%
  mutate(across(8:82, ~ parse_number(as.character(.))))

england_ks4final <- read_csv(here("sessions", "L6_data", "Performancetables_130242", "2022-2023", "england_ks4final.csv"), na = c("", "NA", "SUPP", "NP", "NE", "SP", "LOWCOV", "NEW")) %>%
  mutate(URN = as.character(URN)) %>%
  mutate(across(TOTPUPS:PTOTENT_E_COVID_IMPACTED_PTQ_EE, ~ parse_number(as.character(.))))

england_school_information <- read_csv(
  here("sessions", "L6_data", "Performancetables_130242", "2022-2023", "england_school_information.csv"),
  na = c("", "NA", "SUPP", "NP", "NE", "SP"),
  col_types = cols(
    URN = col_character(),
    OFSTEDLASTINSP = col_date(format = "%d-%m-%Y")  # Adjust format if needed
  )
)


abs_meta <- read_csv(here("sessions", "L6_data",  "Performancetables_130249", "2022-2023", "abs_meta.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()
census_meta <- read_csv(here("sessions", "L6_data",  "Performancetables_130249", "2022-2023", "census_meta.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()
ks4_mats_performance_meta <- read_csv(here("sessions", "L6_data",  "Performancetables_130249", "2022-2023", "ks4-mats-performance_meta.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()
ks4_pupdest_meta <- read_csv(here("sessions", "L6_data",  "Performancetables_130249", "2022-2023", "ks4-pupdest_meta.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()
ks4final_meta <- read_xlsx(here("sessions", "L6_data",  "Performancetables_130249", "2022-2023", "ks4_meta.xlsx"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()
school_information_meta <- read_csv(here("sessions", "L6_data",  "Performancetables_130249", "2022-2023", "la_and_region_codes_meta.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()
la_and_region_codes_meta <- read_csv(here("sessions", "L6_data",  "Performancetables_130249", "2022-2023", "la_and_region_codes_meta.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()
school_information_meta <- read_csv(here("sessions", "L6_data",  "Performancetables_130249", "2022-2023", "school_information_meta.csv"), na = c("", "NA", "SUPP", "NP", "NE")) %>% 
  clean_names()

# str(england_abs)
# str(england_census)
# str(england_ks4_mats_performance)
# str(england_ks4_pupdest)
# str(england_ks4final)
# str(england_school_information)
# str(abs_meta)
# str(census_meta)
# str(ks4_mats_performance_meta)
# str(ks4_pupdest_meta)
# str(ks4final_meta)
# str(school_information_meta)
# str(la_and_region_codes_meta)


```

```{r}

# Custom join function to drop duplicate columns (except URN)
# safe_left_join <- function(x, y) {
#   common_cols <- intersect(names(x), names(y))
#   common_cols <- setdiff(common_cols, "URN")  # keep URN
#   y_clean <- y |> select(-all_of(common_cols))
#   left_join(x, y_clean, by = "URN")
# }

# Perform sequential joins
# england_school_2022_23 <- safe_left_join(england_ks4final) |>
#   safe_left_join(england_abs) |>
#   safe_left_join(england_census) |>
#   safe_left_join(england_ks4_pupdest) |>
#   safe_left_join(england_school_information)

# Left join england_ks4final with england_abs
england_school_2022_23 <- england_ks4final %>%
  left_join(england_abs, by = "URN") %>%
  left_join(england_census, by = "URN") %>%
  left_join(england_school_information, by = "URN")

# Filter out special schools and those with ADMPOL (admissions policy) = "NSE" (non-selective)

england_school_2022_23 <- england_school_2022_23 |>
  left_join(edubase_schools, by = c("URN" = "urn"))

england_school_2022_23_not_special <- england_school_2022_23 %>%
  filter(MINORGROUP != "Special school")

#column_headers_df <- tibble(column_name = names(england_school_2022_23))
  
```

## This week's Session - Foundations {transition="convex-in none-out" transition-speed="fast"}

```{r, echo=FALSE}
# This makes the fonts play nicely within the figures
knitr::opts_chunk$set(dev = "ragg_png")

```

```{css}
/* This sits here, because it allows us to use images from within the L6_images/ folder. Otherwise, the file structure gets a lot more involved! */
.reveal::after {
  background-image: url('L6_images/light-background.png');
}
```

Motivation:

-   Over the years I've marked too many dissertations and examined too many PhDs where people still get the basics of regression wrong - this is my attempt to help fix this once and for all!
-   I've been using this method for 20 years and I ***still*** get things wrong and I'm ***still*** learning new things about it!
-   Once you get the basics, it is perhaps *THE* most useful tool in your statistical toolbox
-   Forget about machine learning and pointlessly complicated models - a simple regression model is your statistical Swiss Army Knife!

## This week's Session - Foundations {transition="convex-in none-out" transition-speed="fast"}

::::: columns
::: {.column width="60%"}
-   A Recipe for Success:
    -   understanding your ingredients (data)
    -   understanding the basic method (rules) - sample size, heteroscedascity, variance, degrees of freedom
    -   baking your first cake (model)
    -   judging your efforts (interpreting the outputs)
:::

::: {.column width="40%"}
![](https://gu-witness.s3.amazonaws.com/media/31ab0d58-3f7a-44d0-842e-adfcf80f1d8c-mediumoriginalaspectdouble.jpg)
:::
:::::

## This week's Session - Foundations {transition="convex-in none-out" transition-speed="fast"}

-   Applied Regression in the real world
    -   Any model is just a simplified version of reality
    -   A good model can offer you insights into the real world situation you are studying and can underpin good real world decisions
    -   a good model interpreted badly can lead to poor policy decisions 
    -   a bad model, even if interpreted correctly, will probably also lead to poor policy decisions 
-   So we need to get both the modelling and the interpretation correct!

# Our Real World Case Study - What are the Factors Affecting School-Level Educational Attainment in Brighton and Hove, England? {background-image="https://adamdennett.github.io/BH_Schools_Consultation/attainment_extra_files/figure-html/unnamed-chunk-10-1.png" background="#2e6260" background-opacity="0.1"}

## Secondary Schools and Attainment - GCSEs {transition="convex-in none-out" transition-speed="fast"}

-   Secondary Schools mainly teach children between the ages of 11-16 in England and Wales (some 11-18, some 13-18)
-   The examinations most children take at the end of Year 11 (age 16) are called GCSEs (General Certificate of Secondary Education)
-   The GCSEs are graded from 9 (highest) to 1 (lowest), with a grade of 4 considered a "standard pass" and a grade of 5 considered a "strong pass"

## Secondary Schools and Attainment - Attainment 8 {transition="convex-in none-out" transition-speed="fast"}

-   Attainment 8 is a measure that sums the grades for each pupil across 8 GCSEs (the standard number taken).
    -   Maths is always counted twice and English often counted twice where both language and literature are taken.
    -   Thus a maximum Attainment 8 score of 90 can be achieved
    -   40 = Standard Pass, 50 = Strong Pass
-   The Attainment 8 scores for all year 11 students can be averaged for each school giving a school-level average Attainment 8 Score
-   Attainment 8 is a raw score and doesn't account for important variations in the cohorts of students each school admits or the types of school, so direct comparison between schools without accounting for these factors is risky

## Secondary Schools and Attainment - Progress 8 {transition="convex-in none-out" transition-speed="fast"}

-   Progress 8 is an alternative attainment score which looks at the progress a student makes between arriving at a school in year 7 or 9 and leaving at age 16
-   It compares their levels of attainment at entry and exit with the progress made by similar students nationally
-   Progress 8 is a 'value-added' ratio. A score of zero means students, on average, made expected progress, while a positive score means they made more progress than expected, and a negative score means they made less

## Secondary Schools, Attainment and Urban Policy {transition="convex-in none-out" transition-speed="fast"}

::::: columns
::: {.column width="60%"}
-   School performance and pupil attainment can be a big urban policy issue - particularly where variations in access and perceived quality occur
-   These variations feed into broader socio-economic issues in cities
-   In the UK, schools are the responsibility of local government
-   Understanding the drivers behind pupil attainment and school performance vital for effective resource allocation and good local policy
:::

::: {.column width="40%"}
![](L6_images/BHCC_Engagement_PPT.png)
:::
:::::

::: footer
:::

## Secondary Schools, Attainment and Urban Policy {transition="convex-in none-out" transition-speed="fast"}

::::: columns
::: {.column width="60%"}
-   In 2024, Brighton and Hove Council convinced pupil attainment mainly driven by ***Disadvantage Attainment Gap*** - socio-economically disadvantaged students perform worse than their more affluent peers
-   Work of Professor Stephen Gorard, University of Durham, suggests mixing of disadvantage improves attainment
-   **Solution**: create more socially mixed schools through a new controversial admissions policy
:::

::: {.column width="40%"}
![](L6_images/gorard_conversation.png)
:::
:::::

::: footer
The Conversation: <https://theconversation.com/poorer-pupils-do-worse-at-school-heres-how-to-reduce-the-attainment-gap-205535>
:::

## Reserarch Question(s) {transition="convex-in none-out" transition-speed="fast"}

-   What are the factors that affect school-level educational attainment in Brighton and Hove?
-   To what extent is attainment driven by social mixing?
-   Are there any other factors that are relevant?
-   What are the implications of this for local policy?
-   Can regression help us and, if it can, how can we go about carefully building a regression model to help us answer these questions?

# A Reliable Regression Modelling Recipe {background-image="https://adamdennett.github.io/BH_Schools_Consultation/attainment_extra_files/figure-html/unnamed-chunk-10-1.png" background="#2e6260" background-opacity="0.1"}

## Step 1 - Ingredients {transition="convex-in none-out" transition-speed="fast"}

-   The **Exploratory Data Analysis** phase is the most important part of any modelling exercise (**see Weeks 1 & 2 of this course**)
-   Failure to get to know your data properly means you might mis-specify your model by:
    -   using the wrong explanatory variables or omitting some key ones
    -   misunderstanding your data types - e.g. counts vs continuous
    -   misunderstanding the relationships between your variables (linear vs logarithmic)
    -   not accounting for important spatial or temporal patterns (autocorrelation) that might mean your observations are not independent

## Step 1a - Ingredients (gathering and preparation) {transition="convex-in none-out" transition-speed="fast"}

::::: columns
::: {.column width="60%"}
-   The data is collected by the Department for Education (DfE) - a full annual census of each school's 
-   Hundreds of variables collected relating to:
    -   attainment and progress
    -   pupil characteristics
    -   school characteristics
-   Some data / variables (ingredients) will be more useful than others
:::

::: {.column width="40%"}
![](L6_images/gov_school_data.png)
:::
:::::

::: footer
DfE Data: <https://www.compare-school-performance.service.gov.uk/compare-schools>
:::

## Step 1a - Ingredients (gathering and preparation) {transition="convex-in none-out" transition-speed="fast"}

```{r}
library(reactable)
library(casaviz)  # assuming casa_reactable_theme is defined here

england_school_2022_23 %>% 
  filter(LANAME == "Brighton and Hove") %>% 
  filter(phase_of_education_name == "Secondary") %>% 
  filter(establishment_status_name == "Open") %>% 
  select(URN, SCHNAME.x, TOWN.x, TOTPUPS, ATT8SCR, OFSTEDRATING, MINORGROUP, PTFSM6CLA1A) %>%
  reactable(theme = casa_reactable_theme(colour = "purple"))


```

## Step 1b - Ingredients (familiarisation) {transition="convex-in none-out" transition-speed="fast"}

```{r}
library(dplyr)

filtered_df <- england_school_2022_23 %>%
  filter(LANAME == "Brighton and Hove",
         phase_of_education_name == "Secondary",
         establishment_status_name == "Open") %>%
  select(URN, SCHNAME.x, TOWN.x, TOTPUPS, ATT8SCR, OFSTEDRATING, MINORGROUP, PTFSM6CLA1A, easting, northing)

library(sf)

# Convert to sf object with EPSG:27700
sf_df <- st_as_sf(filtered_df, coords = c("easting", "northing"), crs = 27700)

# Transform to EPSG:4326
sf_df <- st_transform(sf_df, crs = 4326)

# Extract lat/lon for leaflet
sf_df <- sf_df %>%
  mutate(
    lon = st_coordinates(.)[,1],
    lat = st_coordinates(.)[,2]
  )

library(leaflet)
library(scales)

# Define size and color scales
size_scale <- rescale(sf_df$TOTPUPS, to = c(4, 12))  # radius from 4 to 12
# Create custom color palette
casa_palette <- casa_palettes$default
color_scale <- colorNumeric(palette = casa_palette, domain = sf_df$TOTPUPS)


leaflet(sf_df) |>
  addProviderTiles("CartoDB.Positron") |>  # plain, minimal basemap
  addCircleMarkers(
    lng = ~lon,
    lat = ~lat,
    radius = size_scale,
    color = ~color_scale(TOTPUPS),
    stroke = FALSE,
    fillOpacity = 0.8,
    popup = ~paste0(
      "<strong>", SCHNAME.x, "</strong><br>",
      "Pupils: ", TOTPUPS
    )
  ) |> 
  addLegend(
    "bottomright",
    pal = color_scale,
    values = ~TOTPUPS,
    title = "Total Pupils",
    opacity = 0.7
  )
```

-   Visualisation is perhaps the most important part of the EDA phase
-   Always map and graph your data so that you can spot potential issues before they ruin your model!

## Step 1b - Ingredients (familiarisation) {transition="convex-in none-out" transition-speed="fast"}

```{r}
#| label: attainment-8-histogram
#| fig.width: 8
#| fig.height: 5
#| fig.align: "center"

median_value <- median(england_school_2022_23$ATT8SCR, na.rm = TRUE)
mean_value   <- mean(england_school_2022_23$ATT8SCR, na.rm = TRUE)
sd_value   <- sd(england_school_2022_23$ATT8SCR, na.rm = TRUE)

ggplot(england_school_2022_23, aes(x = ATT8SCR)) +
  geom_histogram(aes(y = ..density..), binwidth = 1, fill = "#4E3C56", alpha = 0.4) +
  stat_function(fun = dnorm, args = list(mean = mean_value, sd = sd_value),
                color = "#2E6260", size = 1) +
  geom_vline(xintercept = median_value, color = "black", linetype = "dotted", size = 1) +
  geom_vline(xintercept = mean_value, color = "#F9DD73", linetype = "solid", size = 1) +
  annotate("text",
           x = median_value, y = Inf,
           label = paste0("Median = ", round(median_value, 1)),
           vjust = 1.3, color = "black", size = 3.5) +
  annotate("text",
           x = mean_value, y = Inf,
           label = paste0("Mean = ", round(mean_value, 1)),
           vjust = 30.5, color = "#F9DD73", size = 3.5) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.08))) +
  labs(
    title = "Attainment 8 - All Schools England and Wales, 2022/23 Academic Year",
    x = "Attainment 8 Score",
    y = "Density"
  ) +
  theme_minimal()


```

-   Hmmm? Are there any problems that could be indicated here?

## Step 1b - Ingredients (familiarisation) {transition="convex-in none-out" transition-speed="fast"}

```{r}
#| label: attainment-8-boxplot
#| fig.width: 8
#| fig.height: 5
#| fig.align: "center"

library(ggplot2)
library(casaviz)  # ensure the package is loaded so its scale functions are available

median_value <- median(england_school_2022_23$ATT8SCR, na.rm = TRUE)

ggplot(england_school_2022_23, aes(x = ATT8SCR, y = "")) +
  geom_boxplot(fill = "#EDD971", alpha = 0.1, outlier.shape = NA) +    
  geom_jitter(aes(colour = MINORGROUP), height = 0.2, alpha = 0.8, size = 1) + 
  scale_colour_casa() +  # applies the default casaviz discrete palette
  labs(
    title = "Attainment 8 - All Schools 2022/23 Academic Year",
    x = "Attainment 8 Score",
    y = NULL,
    colour = "School Type"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.key.size = unit(10, "mm"),   # increase dot space
    legend.text = element_text(size = 10)  # optional: larger legend labels
  ) + guides(colour = guide_legend(override.aes = list(size = 4)))
```

-   Notice anything now?
-   What could/should we do about it? Any suggestions?

## Step 1c - Ingredients (selection) {transition="convex-in none-out" transition-speed="fast"}

```{r}
library(ggplot2)
library(dplyr)

# Filter out unwanted categories
england_filtered <- england_school_2022_23 %>%
  filter(!MINORGROUP %in% c("Special school", "Independent school", "College", NA))

#write_csv(england_filtered, "data/england_filtered.csv")

# Recalculate stats on the filtered data
median_value <- median(england_filtered$ATT8SCR, na.rm = TRUE)
mean_value   <- mean(england_filtered$ATT8SCR, na.rm = TRUE)

ggplot(england_filtered, aes(x = ATT8SCR, y = "")) +
  geom_boxplot(fill = "#EDD971", alpha = 0.1, outlier.shape = NA) +    
  geom_jitter(aes(colour = MINORGROUP), height = 0.2, alpha = 0.5, size = 1) + 
  # Median + mean lines
  # Custom fixed palette matching previous graph
  scale_colour_manual(
    values = c(
      "Academy"          = "#2E6260",
      "Maintained school" = "#E16FCA"
    )
  ) +
  labs(
    title = "Attainment 8 - Academy and Maintained Schools 2022/23 Academic Year",
    x = "Attainment 8 Score",
    y = NULL,
    colour = "School Type"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.key.size = unit(4, "mm")
  ) +
  guides(colour = guide_legend(override.aes = list(size = 3)))
```

## Step 1c - Ingredients (selection) {transition="convex-in none-out" transition-speed="fast"}

```{r}
#| label: attainment-8-histogram2
#| fig.width: 8
#| fig.height: 5
#| fig.align: "center"

library(ggplot2)
library(dplyr)

# Filter data
england_filtered <- england_school_2022_23 %>%
  filter(!MINORGROUP %in% c("Special school", "Independent school", "College", NA))

# Recalculate stats for filtered data
median_value <- median(england_filtered$ATT8SCR, na.rm = TRUE)
mean_value   <- mean(england_filtered$ATT8SCR, na.rm = TRUE)
sd_value   <- sd(england_filtered$ATT8SCR, na.rm = TRUE)

ggplot(england_filtered, aes(x = ATT8SCR)) +
  geom_histogram(aes(y = ..density..), binwidth = 1, fill = "#4E3C56", alpha = 0.4) +
  stat_function(fun = dnorm, args = list(mean = mean_value, sd = sd_value),
                color = "#2E6260", size = 1) +
  geom_vline(xintercept = median_value, colour = "black", linetype = "dotted", size = 1) +
  geom_vline(xintercept = mean_value, colour = "#F9DD73", linetype = "solid", size = 1) +
  annotate("text",
           x = median_value, y = Inf,
           label = paste0("Median = ", round(median_value, 1)),
           vjust = 1.3, colour = "black", size = 3.5) +
  annotate("text",
           x = mean_value, y = Inf,
           label = paste0("Mean = ", round(mean_value, 1)),
           vjust = 30.5, colour = "#F9DD73", size = 3.5) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.08))) +
  labs(
    title = "Attainment 8 - Academy and Maintained Schools 2022/23 Academic Year",
    x = "Attainment 8 Score",
    y = "Density"
  ) +
  theme_minimal()
```

## Step 1c - Ingredients (selection) {transition="convex-in none-out" transition-speed="fast"}

```{r}
#| label: attainment-8-histogram3
#| fig.width: 8
#| fig.height: 5
#| fig.align: "center"

library(ggplot2)
library(dplyr)

# Filter data
england_filtered <- england_school_2022_23 %>%
  filter(!MINORGROUP %in% c("Special school", "Independent school", "College", NA))

# Recalculate stats for filtered data
median_value <- median(england_filtered$PTFSM6CLA1A, na.rm = TRUE)
mean_value   <- mean(england_filtered$PTFSM6CLA1A, na.rm = TRUE)
sd_value   <- sd(england_filtered$PTFSM6CLA1A, na.rm = TRUE)

ggplot(england_filtered, aes(x = PTFSM6CLA1A)) +
  geom_histogram(aes(y = ..density..), binwidth = 1, fill = "#4E3C56", alpha = 0.4) +
  stat_function(fun = dnorm, args = list(mean = mean_value, sd = sd_value),
                color = "#2E6260", size = 1) +
  geom_vline(xintercept = median_value, colour = "black", linetype = "dotted", size = 1) +
  geom_vline(xintercept = mean_value, colour = "#F9DD73", linetype = "solid", size = 1) +
  annotate("text",
           x = median_value, y = Inf,
           label = paste0("Median = ", round(median_value, 1)),
           vjust = 1.3, colour = "black", size = 3.5) +
  annotate("text",
           x = mean_value, y = Inf,
           label = paste0("Mean = ", round(mean_value, 1)),
           vjust = 30.5, colour = "#F9DD73", size = 3.5) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.08))) +
  labs(
    title = "% Disadvantaged Students - Academy and Maintained Schools 2022/23 Academic Year",
    x = "% Disadvantaged",
    y = "Density"
  ) +
  theme_minimal()
```

## Step 1c - Ingredients (selection) {transition="convex-in none-out" transition-speed="fast"}

::::: columns
::: {.column width="50%"}
-   The process of exploring and interrogating your data should take much longer than the modelling at the end
-   The process is iterative - you might well be exploring your data and the theory ***at the same time*** to help you explore, filter, select and prepare for the modelling phase
:::

::: {.column width="50%"}
```{mermaid}
flowchart TD
    A["<img src='L6_images/Data.png'>"] --> B[<img src='L6_images/Book.png'>]
    B[<img src='L6_images/Book.png'>] --> A["<img src='L6_images/Data.png'>"]
    
```
:::
:::::

## Step 1c - Ingredients (selection) {transition="convex-in none-out" transition-speed="fast"}

::::: columns
::: {.column width="50%"}
-   Understanding your 'system' is vital
-   What is "in" and what is "out"?
-   How do you know?
    -   Experience (e.g. I used to be a school teacher so aware of differences in types of school)
    -   Research! If you are unfamiliar with the system, do your research
:::

::: {.column width="50%"}
[![](L6_images/being_interdisciplinary.jpg){width="50%"}](https://uclpress.co.uk/book/being-interdisciplinary/)
:::
:::::

## Step 1c - Ingredients (selection) {transition="convex-in none-out" transition-speed="fast"}

-   The role of theory and wider research on your system is vital in helping you select your variables for investigation
-   **DO NOT JUST THROW EVERYTHING INTO YOUR MODEL JUST BECAUSE YOU HAVE SOME VARIABLES**
    -   This is a common mistake
    -   It can also lead to spurious interpretation where correlation and causation are not the same thing
-   Carrying out a thorough literature review will also give you context for interpreting your model results later on in the process
-   **ALWAYS CARRY OUT A THOROUGH LITERATURE REVIEW TO HELP GUIDE YOUR DATA / VARIABLE SELECTION**

## Step 1c - Ingredients (selection) {transition="convex-in none-out" transition-speed="fast"}

::::: columns
::: {.column width="60%"}
-   Brighton and Hove City Council relied heavily on the work of Gorard in building its policy
-   Paper links social mixing to improved attainment for disadvantaged pupils
-   Includes variables such as:
    -   school type (e.g. academy, maintained)
    -   pupil characteristics (e.g. free school meals (FSM) eligibility, special educational needs, ethnicity)
    -   school characteristics (e.g. size, location)
-   Thus all might be worth investigating
:::

::: {.column width="40%"}
![](L6_images/gorard_disad_paper.png)
:::
:::::

::: footer
<https://journals.sagepub.com/doi/full/10.1177/2158244018825171>
:::

## Step 1c - Ingredients (selection) {transition="convex-in none-out" transition-speed="fast"}

::::: columns
::: {.column width="60%"}
-   However, wider reading also suggests that factors such as attendance (which Gordard does not include in his paper as a variable) may also play a big role in the attainment of disadvantaged pupils
-   Work by Claymore suggests that a large proportion of the gap in attainment between disadvantaged pupils and more affluent peers can be explained by:
    -   the differences in absence rates
    -   exclusion
    -   rates of moving between schools
:::

::: {.column width="40%"}
![](L6_images/nfer.png)
:::
:::::

::: footer
<https://www.nfer.ac.uk/publications/being-present-the-power-of-attendance-and-stability-for-disadvantaged-pupils/>
:::

## Step 2 - Method {transition="convex-in none-out" transition-speed="fast"}

```{mermaid}
flowchart TD
    I{Choosing a <br/>statistical test} -->
    A[How Many Variables?] -->
    C(2?) & G(More than 2?)
    C -->|Categorical?| D(Chi Squared <br/>or Similar e.g. T-test)
    C -->|Scale or Ratio?| E(Pearson Correlation <br/>or Spearman's Rank)
    C -->|Both?| F(Ask Google)
    G --> H(REGRESSION - everything else, <br/>get in the bin)
    
```

<!-- ## Step 2 - Method {transition="convex-in none-out" transition-speed="fast"} -->

<!-- -   ***Based on our raw ingredients, which methods might allow us to try and answer our research question(s)?*** -->

<!-- -   ***Dependent variable(s)*** (what we are trying to explain / predict): Attainment 8 / Progress 8 **-\>** continuous / Ratio scales -->

<!-- -   ***Independent variables*** - explanatory / predictor variables of varying data types to explain variation in the dependent variable -->

<!-- -   As such ***Linear Regression*** is the most appropriate statistical test to employ -->

<!-- ## Step 2 - Method {transition="convex-in none-out" transition-speed="fast"} -->

<!-- -   Good regression models are built slowly and iteratively -->

<!-- -   My preference is to start simply (2 variables) and build complexity (more variables, variations on the method) slowly -->

<!-- -   As with complicated baking recipes, it's important to pay attention to each element of the regression recipe and get each part right -->

## Linear Regression - It's just a scatter plot!

```{r}
#| warning: FALSE
#| out-width: "60%"
#| fig-align: "center"
## get LEA Code from here: https://get-information-schools.service.gov.uk/Guidance/LaNameCodes

# Filter data
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>% 
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A)

#| warning: FALSE
#| out-width: "60%"
#| fig-align: "center"

## get LEA Code from here: https://get-information-schools.service.gov.uk/Guidance/LaNameCodes

# Filter data
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>% 
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A)

# Define custom color palette
custom_colors <- c(
  "#2E6260", "#658E62", "#9DBB65", "#C4CE6A", "#E7D870",
  "#F3C486", "#E993AC", "#D069BD", "#8F5289", "#4E3C56"
)

school_levels <- unique(btn_sub$SCHNAME.x)
school_colors <- setNames(
  rep(custom_colors, length.out = length(school_levels)),
  school_levels
)


# Filter data
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>%
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A)

# Fit linear model and get predicted values
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub)
btn_sub <- btn_sub %>%
  mutate(predicted = predict(lm_fit),
         residual = ATT8SCR - predicted)

# Create annotation text
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)
annotation_text <- paste0("y = ", intercept, " + ", slope, "x")

# Assign colors to schools
school_levels <- unique(btn_sub$SCHNAME.x)
school_colors <- setNames(
  rep(custom_colors, length.out = length(school_levels)),
  school_levels
)

# Plot with residual lines and custom colors
ggplot(btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR, color = SCHNAME.x)) +
  geom_point(size = 3, alpha = 0.9) +
  labs(
    title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
    x = "% Disadvantaged Students",
    y = "Average Attainment 8 Score",
    color = "School"
  ) +
  scale_color_manual(values = school_colors) +
  theme_minimal() +
  theme(
    legend.position = c(1, 1),           # top-right inside plot
    legend.justification = c(1, 1)
  )
```

-   **A regression model is nothing more than a description of a scatter plot**
-   Dependent variable = $Y$-axis
-   Independent variable = $X$-axis

## Linear Regression - Line of Best-fit

```{r}
#| out-width: "60%"
#| fig-align: "center"
#| 
## get LEA Code from here: https://get-information-schools.service.gov.uk/Guidance/LaNameCodes

# Filter data
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>% 
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A)

# Fit linear model
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub)
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)

# Create annotation text
annotation_text <- paste0("y = ", intercept, " + ", slope, "x")

# Plot with annotation
ggplot(btn_sub) +
  geom_point(aes(x = PTFSM6CLA1A, y = ATT8SCR)) +
  geom_smooth(aes(x = PTFSM6CLA1A, y = ATT8SCR), method = "lm", se = FALSE, color = "#8F5289") +
  labs(title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  theme_minimal()
```

-   The linear regression model is the straight line of best-fit
-   A linear model function `lm()` (in `R`) uses a method called ***Ordinary Least Squares (OLS)*** to find the line of best-fit.
-   The best line minimises the squared (so that negatives and positives don't cancel) vertical distances between the line and the points - hence OLS

## Linear Regression - Residuals / Error

```{r}
#| out-width: "60%"
#| fig-align: "center"

# Filter data (assuming england_filtered is in your environment)
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>% 
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A)

# Fit linear model
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub)

# Get the R-squared value from the model summary
r2_value <- summary(lm_fit)$r.squared

# Create the annotation text with the R-squared value, rounded to 2 decimal places
annotation_text <- paste0("R² = ", round(r2_value, 2))

# Plot with residual lines and the new annotation
ggplot(btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "#8F5289") +
  geom_segment(aes(xend = PTFSM6CLA1A, yend = predict(lm_fit)),
               linetype = "dotted", color = "grey50") +
  # Use the new R-squared annotation
  annotate("text", 
           x = max(btn_sub$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(btn_sub$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, 
           hjust = 1, vjust = -10, size = 4, color = "#4E3C56") +
  labs(title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  theme_minimal()
```

-   The vertical distances between the points and the line of best fit are called the **residuals** - sometimes also referred to as the **errors** or $\epsilon$
-   The closer the points are to the line, the better the fit of the model

## Linear Regression - R-Squared

```{r}
#| out-width: "60%"
#| fig-align: "center"

# Filter data (assuming england_filtered is in your environment)
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>% 
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A)

# Fit linear model
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub)

# Get the R-squared value from the model summary
r2_value <- summary(lm_fit)$r.squared

# Create the annotation text with the R-squared value, rounded to 2 decimal places
annotation_text <- paste0("R² = ", round(r2_value, 2))

# Plot with residual lines and the new annotation
ggplot(btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "#8F5289") +
  geom_segment(aes(xend = PTFSM6CLA1A, yend = predict(lm_fit)),
               linetype = "dotted", color = "grey50") +
  # Use the new R-squared annotation
  annotate("text", 
           x = max(btn_sub$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(btn_sub$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, 
           hjust = 1, vjust = -10, size = 4, color = "#4E3C56") +
  labs(title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  theme_minimal()
```

-   The fit of the model is represented by the ***coefficient of determination*** or $R^2$ value (0-1), which is calculated from the residuals
-   It describes how much of the variation in $Y$ (Attainment 8 score) is explained by the variation in $X$ (% Disadvantaged Students) - here 69%
-   The closer to 1 (100%), the better the fit of the model

## Linear Regression - R-Squared

![](L6_images/Correlation_examples2.png)

-   It's easy to visually estimate your $R^2$ value from looking at how well correlated the points are. *NB as squared, always +*

::: footer
Source: <https://work.thaslwanter.at/Stats/html/statsRelation.html>
:::

## Linear Regression - Slope and Intercept

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 60%

# Filter data
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>% 
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A)

# Fit linear model and get predicted values
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub)
btn_sub <- btn_sub %>%
  mutate(predicted = predict(lm_fit),
         residual = ATT8SCR - predicted)


# Create annotation text
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)
annotation_text <- paste0("y = ", intercept, " + ", slope, "x")

ggplot(btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR)) +
  geom_point() +
  geom_abline(intercept = coef(lm_fit)[1], slope = coef(lm_fit)[2], color = "#8F5289") +
  geom_segment(aes(xend = PTFSM6CLA1A, yend = predicted),
               linetype = "dotted", color = "grey50") +
  annotate("text", x = max(btn_sub$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(btn_sub$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, hjust = 1, vjust = -15, size = 4, color = "#4E3C56") +
  labs(title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  xlim(0, max(50)) +
  ylim(min(30), 65) +
  theme_minimal()

```

-   The regression line itself can be described by an equation with two parameters / coefficients:
    -   The **intercept** - $\beta_0$ - which is the value of $Y$ when $X = 0$
    -   The **slope** - $\beta_1$ - which the change in the value of $Y$ for a 1 unit change in $X$
    
## Linear Regression - Slope and Intercept

-   Another way of thinking of your **intercept** is as your *model baseline*. 
    -   In our case: baseline attainment, controlling for disadvantage.
-    Higher or lower intercept values for different local authorities relate to higher or lower baseline attainment. 
-   Another way of thinking about your **slope** is the level of influence your $X$ variable might be having on your $Y$ variable
    -   Higher value = Steeper Slope = More influence
    -   Lower value = more horizontal slope = Less influence

## Linear Regression - Model Estimates

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 60%

# Filter data
btn_sub <- england_filtered %>%
  filter(LEA == 846) %>% 
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A, PERCTOT, OFSTEDRATING, gor_name)

# Fit linear model and get predicted values
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub)
btn_sub <- btn_sub %>%
  mutate(predicted = predict(lm_fit),
         residual = ATT8SCR - predicted)


# Create annotation text
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)
annotation_text <- paste0("y = ", intercept, " + ", slope, "x")

ggplot(btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR)) +
  geom_point() +
  geom_abline(intercept = coef(lm_fit)[1], slope = coef(lm_fit)[2], color = "#8F5289") +
  geom_segment(aes(xend = PTFSM6CLA1A, yend = predicted),
               linetype = "dotted", color = "grey50") +
  annotate("text", x = max(btn_sub$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(btn_sub$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, hjust = 1, vjust = -15, size = 4, color = "#4E3C56") +
  geom_point(aes(x = 35, y = 40.3), color = "#e16fca", size = 4) +  # Red dot added here
  labs(title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  xlim(0, max(50)) +
  ylim(min(30), 65) +
  theme_minimal()


```

$$Y = \beta_0 + \beta_1X_1 + \epsilon$$ $$\hat{Y} = 62.35 + (-0.63 \times X) + \epsilon$$ [$$40.3 = 62.35 + (-0.63 \times 35) + 0$$]{style="color:#e16fca"}

## Linear Regression - The Statistical Model

::::: columns
::: {.column width="50%"}
-   Scatter plots are an excellent intuitive way to understand the relationship between two continuous variables
-   But algorithms are required to generate the various plot statistics and other useful info
-   Lots of different statistical software packages will do this
-   Doesn't matter which you use, they all do pretty much the same thing under the hood
:::

::: {.column width="50%"}
![](L6_images/Python-logo-notext.svg.png){width="20%"} ![](L6_images/R_logo.svg.png){width="20%"} ![](L6_images/Stata_logo_med_blue.png){width="20%"} ![](L6_images/spss-1-logo-png-transparent.png){width="20%"} ![](L6_images/statsmodels-logo-v2-horizontal.svg){width="20%"} ![](L6_images/tidy_models.png){width="20%"}
:::
:::::

## Linear Regression - The Statistical Model

```{r}
#| out-width: "40%"
#| out-height: "50%"
#| fig-align: "center"
summary(lm_fit)
```

-   Example output from R

## Linear Regression - Running the Model

![](L6_images/the_Call.png)

-   This is the code representation of the model equation we saw earlier:
    -   `lm()` is the function that fits a linear model
    -   `ATT8SCR` (Attainment 8 Score) is the dependent variable $Y$
    -   `~` means "is modelled by"
    -   `PTFSM6CLA1A` (% Disadvantaged Students) is the independent variable $X$
    -   `data = bnt_sub` is the dataset we are using which contains the variables

## Linear Regression - Residuals and Error

![](L6_images/the_resids.png) ![](L6_images/the_error.png)

-   The residual errors range from -8.3 (Longhill) to + 4.7 (Varndean)
-   The residual standard error of 4.087 = predictions for School's Attainment 8 score are off by about 4.1 points
-   Residual Standard Error (RSE) and $R^2$ are inversely related - A smaller RSE and larger $R^2$ both mean a more precise model
-   The F-Statistic is a ratio of the amount of variance in $Y$ explained by the model to that not. x17.7 more - statistically significant \<0.005

## Linear Regression - Degrees of Freedom

![](L6_images/the_error.png)

-   DF are the ***Degrees of Freedom*** in the model. DF ***very*** important for understanding how reliable your $R^2$ value might be
    -   The first number (1) relates to the number of variables
    -   The second number (8) relates to the number of observations (cases) in the dataset, minus the number of parameters
    -   In our example we have 10 observations and 2 parameters (intercept and slope) so $10 - 2 = 8$ degrees of freedom

## Linear Regression - Degrees of Freedom

-   **General rule**: ***more degrees of freedom = more reliable model***
-   A model with many parameters vs observations will bend to fit those observations and not be a good generalisation - this is called ***overfitting***
-   A model with little freedom might *appear* to have a high $R^2$
    -   but it is likely to perform poorly on new, unseen data
    -   it has captured the characteristics of your specific dataset rather than an underlying truth
-   **General Rule**: ***the more observations in your dataset the better!***
-   **High** $R^2$ with low DF = :poop:

## Linear Regression - Degrees of Freedom

```{r}
# Filter data: Brighton and Hove (LEA 846), then keep first two rows
btn_sub1 <- england_filtered %>%
  filter(LEA == 846) %>%
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A) %>%
  slice(1:2)

# Fit linear model
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub1)
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)
r_squared <- round(summary(lm_fit)$r.squared, 3)

# Create annotation text
annotation_text <- paste0("y = ", intercept, " + ", slope, "x\nR² = ", r_squared)

# Plot with annotation
ggplot(btn_sub1) +
  geom_point(aes(x = PTFSM6CLA1A, y = ATT8SCR), size = 3, color = "#2C7BB6") +
  geom_smooth(aes(x = PTFSM6CLA1A, y = ATT8SCR), method = "lm", se = FALSE, color = "#8F5289") +
  annotate("text",
           x = mean(btn_sub1$PTFSM6CLA1A),
           y = max(btn_sub1$ATT8SCR),
           label = annotation_text,
           hjust = 0, vjust = 2, size = 5, fontface = "italic") +
  labs(
    title = "Brighton and Hove Schools – \nAttainment 8 vs % Disadvantaged Students, 2022–23",
    x = "% Disadvantaged Students",
    y = "Average Attainment 8 Score"
  ) +
  theme_minimal()
```

-   2 observations - 2 parameters = 0 degrees of freedom

## Linear Regression - Degrees of Freedom

```{r}
# Filter data: Brighton and Hove (LEA 846), then keep first two rows
btn_sub1 <- england_filtered %>%
  filter(LEA == 846) %>%
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A) %>%
  slice(1:3)

# Fit linear model
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub1)
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)
r_squared <- round(summary(lm_fit)$r.squared, 3)

# Create annotation text
annotation_text <- paste0("y = ", intercept, " + ", slope, "x\nR² = ", r_squared)

# Plot with annotation
ggplot(btn_sub1) +
  geom_point(aes(x = PTFSM6CLA1A, y = ATT8SCR), size = 3, color = "#2C7BB6") +
  geom_smooth(aes(x = PTFSM6CLA1A, y = ATT8SCR), method = "lm", se = FALSE, color = "#8F5289") +
  annotate("text",
           x = mean(btn_sub1$PTFSM6CLA1A),
           y = max(btn_sub1$ATT8SCR),
           label = annotation_text,
           hjust = 0, vjust = 2, size = 5, fontface = "italic") +
  labs(
    title = "Brighton and Hove Schools – \nAttainment 8 vs % Disadvantaged Students, 2022–23",
    x = "% Disadvantaged Students",
    y = "Average Attainment 8 Score"
  ) +
  theme_minimal()
```

-   3 observations - 2 parameters = 1 degree of freedom

## Linear Regression - Degrees of Freedom

```{r}
# Filter data: Brighton and Hove (LEA 846), then keep first two rows
btn_sub1 <- england_filtered %>%
  filter(LEA == 846) %>%
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A) %>%
  slice(1:4)

# Fit linear model
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub1)
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)
r_squared <- round(summary(lm_fit)$r.squared, 3)

# Create annotation text
annotation_text <- paste0("y = ", intercept, " + ", slope, "x\nR² = ", r_squared)

# Plot with annotation
ggplot(btn_sub1) +
  geom_point(aes(x = PTFSM6CLA1A, y = ATT8SCR), size = 3, color = "#2C7BB6") +
  geom_smooth(aes(x = PTFSM6CLA1A, y = ATT8SCR), method = "lm", se = FALSE, color = "#8F5289") +
  annotate("text",
           x = mean(btn_sub1$PTFSM6CLA1A),
           y = max(btn_sub1$ATT8SCR),
           label = annotation_text,
           hjust = 0, vjust = 2, size = 5, fontface = "italic") +
  labs(
    title = "Brighton and Hove Schools – \nAttainment 8 vs % Disadvantaged Students, 2022–23",
    x = "% Disadvantaged Students",
    y = "Average Attainment 8 Score"
  ) +
  theme_minimal()
```

-   4 observations - 2 parameters = 2 degrees of freedom

## Linear Regression - Degrees of Freedom

```{r}
# Filter data: Brighton and Hove (LEA 846), then keep first two rows
btn_sub1 <- england_filtered %>%
  filter(LEA == 846) %>%
  select(URN, SCHNAME.x, LEA, ATT8SCR, PTFSM6CLA1A) %>%
  slice(1:5)

# Fit linear model
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_sub1)
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)
r_squared <- round(summary(lm_fit)$r.squared, 3)

# Create annotation text
annotation_text <- paste0("y = ", intercept, " + ", slope, "x\nR² = ", r_squared)

# Plot with annotation
ggplot(btn_sub1) +
  geom_point(aes(x = PTFSM6CLA1A, y = ATT8SCR), size = 3, color = "#2C7BB6") +
  geom_smooth(aes(x = PTFSM6CLA1A, y = ATT8SCR), method = "lm", se = FALSE, color = "#8F5289") +
  annotate("text",
           x = mean(btn_sub1$PTFSM6CLA1A),
           y = max(btn_sub1$ATT8SCR),
           label = annotation_text,
           hjust = 0, vjust = 2, size = 5, fontface = "italic") +
  labs(
    title = "Brighton and Hove Schools – \nAttainment 8 vs % Disadvantaged Students, 2022–23",
    x = "% Disadvantaged Students",
    y = "Average Attainment 8 Score"
  ) +
  theme_minimal()
```

-   5 observations - 2 parameters = 3 degrees of freedom

## Linear Regression - Degrees of Freedom

-   **How many Degrees of Freedom are required for a reliable model?**
-   No single, universally agreed-upon number - aim to maintain a healthy ratio of observations to the number of parameters
-   A common rule of thumb is the 10:1 ratio - **10 observations for every 1 parameter** - although this is a rough guide. If in doubt, even up to 20:1 to be safe
-   In our example, we have 10 observations and 2 parameters = 5:1 ratio = ***so a potentially unreliable model - proceed with extreme caution!***

## Linear Regression - Variance

-   Degress of Freedom are also used to calculate the ***variance*** of the model
-   If you ran the same model on different samples of the same population - e.g instead of Brighton, you ran this model on schools in Leeds or Bristol or Liverpool - you would get different values for your coefficients each time
-   How similar these are to each other and more importantly the the whole population (all schools in England and Wales) is the ***variance*** - you want to minimise the variance in your coefficients if you want to generalise your model to the wider population
-   **We will return to the idea of variance in the practical**

## Linear Regression - Coefficients 1

![](L6_images/the_coefs.png)

-   The Estimates ($\beta$)
    -   **Intercept** $\beta_0$ - estimated Attainment 8 value when % Disadvantaged Children in a school is **zero**
    -   PTFSM6CLA1A (**slope** $\beta_1$) - effect of a one-unit change in the % Disadvantaged Children on Attainment 8 - therefore linked to the units of the independent variable and not directly comparable across different variables if measured on different scales

## Linear Regression - Coefficients 2

![](L6_images/the_coefs.png)

-   The Standard Error (SE) - measure of the uncertainty or precision of the estimate (coefficient) - how much it might vary
    -   **Large SE relative to the Estimate = estimate unreliable / uncertain**
    -   SE of 0.15 for the estimate of -0.63 for PTFSM6CLA1A means the change in the value of Attainment 8 for a 1% change in disadvantaged students could vary between -0.48 and -0.78

## Linear Regression - Coefficients 3

![](L6_images/the_coefs.png)

-   The t-value - ratio of the $\frac{Estimate}{SE}$
    -   a large standard error will make the t-value small (close to zero)
    -   ***t-values can be thought of as a standardised coefficient*** - very useful for comparing the relative importance of different predictors in the model
    -   The **larger the t-value, the more important the predictor** is in explaining the variation in the dependent variable - more important the variable

## Linear Regression - Coefficients 4

![](L6_images/the_coefs.png)

-   The $P$-value - $P$-robability of observing a t-value as extreme as the one calculated if there were ***no relationship*** between the independent and the dependent variable
-   A small $P$-value (typically $p$ \< 0.05) indicates \<5% chance that $X$ is not really explaining variation in $Y$
-   The Signif. codes (like \*\*\* and \*\*) are just a quick visual guide to this p-value, showing you at a glance which variables are significant.
-   Any parameter with 1, 2 or 3\* is "statistically significant" at 5% or better

## Linear Regression - Coefficients 4

![](L6_images/the_coefs.png)

-   The $P$-values relate to the ***null hypotheses*** that:
    -   The true value of the intercept (baseline attainment) is **zero** when the independent variable (% disadvantage) is **zero**. 
    -   $P$ \<0.001 = \<0.1% $P$-robability that 62.3 occurred by random chance
-   **Note** - It doesn't tell us that when % Disadvantaged is zero, 62.3 will be a *correct* Attainment 8 score, just that real value is unlikely to be zero
    
## Linear Regression - Coefficients 4

![](L6_images/the_coefs.png)

-   The $P$-values relate to the ***null hypotheses*** that:
    -   The true value of the PTFSM6CLA1A slope (relationship between % Disadvantaged and Attainment 8) is zero. 
    $P$- \< 0.01 = \<1% $P$-robability that the relationship observed is by chance. Relationship is likely to be real.    

## Linear Regression - Summary so far

```{r}
#| out-width: "40%"
#| out-height: "50%"
#| fig-align: "center"
ggplot(btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR)) +
  geom_point() +
  geom_abline(intercept = coef(lm_fit)[1], slope = coef(lm_fit)[2], color = "#8F5289") +
  geom_segment(aes(xend = PTFSM6CLA1A, yend = predicted),
               linetype = "dotted", color = "grey50") +
  annotate("text", x = max(btn_sub$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(btn_sub$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, hjust = 1, vjust = -15, size = 4, color = "#4E3C56") +
  labs(title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  xlim(0, max(50)) +
  ylim(min(30), 65) +
  theme_minimal()
```

1.  Variation in % disadvantaged students in schools in Brighton appears to explain about 65-68% of the variation in Attainment 8 at the school level

2.  The % disadvantaged students is a statistically significant predictor and the relationship appears to be linear

3.  A 1% reduction in the number of disadvantaged students in a school appears to be associated with a 0.62 point increase in Attainment 8, and vice versa. **So the council was right? Well, not quite...**

## Linear Regression - Summary so far

4.  The degrees of freedom in the model are small, so model likely to be **over-fitted** and unreliable for generalisation

-   Small numbers of observations (schools in the city) mean that the model is highly sensitive to changes in the data
-   It's also unclear whether there are any other factors that might be correlated with disadvantaged students that might also be influencing Attainment 8 scores and ***confounding (return to this later)*** the apparent relationship we observe
-   ***What are the practical consequences of overfitting?***

## Linear Regression - Overfitting, Outliers and High Leverage Points

-   When we overfit, the model can be overly influenced by ***outliers*** (large residuals) and changes in key ***leverage points*** (points that are far from the mean of the independent variable)
-   What happens to our model if:
    -   a school improvement plan brings the outlier (Longhill) closer to attainment at Hove Park?
    -   Council makes Varndean increase its disadvantaged intake to 30%?
    -   And BACA improves its Attainment 8 to the city average following a huge cash injection from its academy trust?

## Linear Regression - Overfitting, Outliers and High Leverage Points

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 40%

btn_edit <- btn_sub %>%
  mutate(ATT8SCR = ifelse(SCHNAME.x == "Longhill High School", 46, ATT8SCR),
         ATT8SCR = ifelse(SCHNAME.x == "Brighton Aldridge Community Academy", 46, ATT8SCR),
         PTFSM6CLA1A = ifelse(SCHNAME.x == "Varndean School", 30, PTFSM6CLA1A))

# Fit linear model and get predicted values
lm_edit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = btn_edit)
btn_edit <- btn_edit %>%
  mutate(predicted = predict(lm_edit),
         residual = ATT8SCR - predicted)

annotation_edit <- paste0("y = ", round(coef(lm_edit)[1], 2), " + ", round(coef(lm_edit)[2], 2), "x")

ggplot(btn_edit, aes(x = PTFSM6CLA1A, y = ATT8SCR)) +
  geom_point() +
  geom_abline(intercept = coef(lm_edit)[1], slope = coef(lm_edit)[2], color = "#8F5289") +
  geom_segment(aes(xend = PTFSM6CLA1A, yend = predicted),
               linetype = "dotted", color = "grey50") +
  annotate("text", x = max(btn_edit$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(btn_edit$ATT8SCR, na.rm = TRUE), 
           label = annotation_edit, hjust = 1, vjust = -10, size = 4, color = "#4E3C56") +
  labs(title = "Brighton and Hove Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  xlim(0, max(50)) +
  ylim(min(30), 65) +
  theme_minimal()
```

-   Plausible changes to three schools have almost completely removed relationship between % Disadvantaged Students & Attainment 8
-   Overfitting -\> parameters highly sensitive to minor changes to a few key data points
-   The ***closer the best-fit line gets to horizontal***, the closer we get to **NO RELATIONSHIP** between the independent and dependent variables

## Linear Regression - Overfitting, Outliers and High Leverage Points

```{r}
summary(lm_edit)
```

-   Overall $P$-value of model now **statistically insignificant** \>0.1
-   \% Disadvantage in Schools now **statistically insignificant**

## Linear Regression - More Degrees of Freedom

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 60%

# Fit model
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = england_filtered)

# Extract coefficients
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)

# Extract R²
r_squared <- round(summary(lm_fit)$r.squared, 2)

# Create annotation text
annotation_text <- paste0("y = ", intercept, " + ", slope, "x\nR² = ", r_squared)

# Plot with annotation
ggplot(england_filtered) +
  geom_point(aes(x = PTFSM6CLA1A, y = ATT8SCR), alpha = 0.6) +  # Base layer: all schools
  geom_point(data = btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR), 
             color = "#F9DD73", size = 2.5) +  # Overlay: Brighton & Hove schools
  geom_smooth(aes(x = PTFSM6CLA1A, y = ATT8SCR), method = "lm", 
              se = FALSE, color = "#8F5289") +
  annotate("text", x = max(england_filtered$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(england_filtered$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, hjust = 2, vjust = -7, size = 4, color = "#4E3C56") +
  labs(title = "England and Wales Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  theme_minimal()
```

-   Now we have a model that includes all schools in England and Wales
-   Negative association present - appears to confirm Gorard's observation
-   R-Squared is reasonable - 34% of the variation explained

## Linear Regression - More Degrees of Freedom

```{r}
summary(lm_fit)

```

-   We now have 3248 degrees of freedom
-   Intercept and % Disadvantaged Students both highly significant
-   Life is good? ***Wrong... Houston, we have some more problems!!***

## Linear Regression - Other Regression Assumptions

-   I forgot to mention - there are another set of rules to follow to ensure our linear regression is reliable:
    -   **Linearity**: The relationship between the independent and dependent variables is linear
    -   **Homoscedasticity**: Constant variance of residuals across all levels of the independent variable
    -   **Normality of residuals**: Residuals are normally distributed
    -   **No multicollinearity**: Independent variables are not highly correlated
    -   **Independence of residuals**: Errors are not related to each other with each other

## Linear Regression - Linearity?

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 60%

# Fit model
lm_fit <- lm(ATT8SCR ~ PTFSM6CLA1A, data = england_filtered)

# Extract coefficients
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)

# Extract R²
r_squared <- round(summary(lm_fit)$r.squared, 2)

# Create annotation text
annotation_text <- paste0("y = ", intercept, " + ", slope, "x\nR² = ", r_squared)

# Plot with annotation
ggplot(england_filtered) +
  geom_point(aes(x = PTFSM6CLA1A, y = ATT8SCR), alpha = 0.6) +  # Base layer: all schools
  geom_point(data = btn_sub, aes(x = PTFSM6CLA1A, y = ATT8SCR), 
             color = "#F9DD73", size = 2.5) +  # Overlay: Brighton & Hove schools
  geom_smooth(aes(x = PTFSM6CLA1A, y = ATT8SCR), method = "lm", 
              se = FALSE, color = "#8F5289") +
  annotate("text", x = max(england_filtered$PTFSM6CLA1A, na.rm = TRUE), 
           y = min(england_filtered$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, hjust = 2, vjust = -7, size = 4, color = "#4E3C56") +
  labs(title = "England and Wales Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "% Disadvantaged Students",
       y = "Average Attainment 8 Score") +
  theme_minimal()
```

-   Linearity - Does your line go through all the points nicely?
-   Hmmmm - suggestion of non-linearity with big up-tick at the low end of disadvantage

## Linear Regression - Linearity?

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("linearity"))
```

-   Ploting the residuals against the fitted values is one check for linearity
-   The residuals should be randomly scattered around zero - if they are not, it suggests a non-linear relationship
-   The reference line should be horizontal at zero

## Linear Regression - Homoscedasticity (Constant variance)?

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("homogeneity"))
```

-   Issues here too: the residuals are not randomly scattered around zero
-   They are instead displaying ***heteroscdasticity*** (different variance)

## Linear Regression - Normality of Residuals?

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("qq"))
```

-   This is known as a **Q-Q plot** (Quantile-Quantile plot)
-   A number of points not on the line - suggesting the residuals are not normally distributed
-   Many points off line could mean untrustworthy p-values

## Linear Regression - Outliers?

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("outliers"))
```

-   At least we don't have any problems with outliers!

## Linear Regression - Diagnostics

-   **Linearity**: :sob:
-   **Homoscedasticity**: :sob:
-   **Normality of residuals**: :sob:
-   **No multicollinearity**: :clock1130:
-   **Independence of residuals**: :clock1130:
-   Presently our model violates most of the key assumptions that underpin linear regression models. What this means is that in its current form, the relationship between % Disadvantaged Students and Attainment 8 described by the model is not reliable

## Linear Regression - Non-Linearity

$$log(Y) = \beta_0 + \beta_1log(X_1) + \epsilon$$

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 50%

england_filtered <- england_filtered %>%
  filter(PTFSM6CLA1A > 0, ATT8SCR > 0)
england_filtered <- england_filtered %>%
  mutate(log_att8 = log(ATT8SCR),
         log_ptfsm = log(PTFSM6CLA1A))

btn_sub <- btn_sub %>%
  mutate(log_att8 = log(ATT8SCR),
         log_ptfsm = log(PTFSM6CLA1A))

# Fit model
lm_fit <- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A), data = england_filtered)

# Extract coefficients
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)

# Extract R²
r_squared <- round(summary(lm_fit)$r.squared, 2)

# Create annotation text
annotation_text <- paste0("y = ", intercept, " + ", slope, "x\nR² = ", r_squared)

# Plot with annotation
ggplot(england_filtered) +
  geom_point(aes(x = log_ptfsm, y = log_att8, alpha = 0.6)) +  # Base layer: all schools
  geom_point(data = btn_sub, aes(x = log_ptfsm, y = log_att8), 
             color = "#F9DD73", size = 2.5) +  # Overlay: Brighton & Hove schools
  geom_smooth(aes(x = log_ptfsm, y = log_att8), method = "lm", 
              se = FALSE, color = "#8F5289") +
  annotate("text", x = max(england_filtered$log_ptfsm, na.rm = TRUE), 
           y = min(england_filtered$log_att8, na.rm = TRUE), 
           label = annotation_text, hjust = 3, vjust = -2, size = 4, color = "#4E3C56") +
  labs(title = "England and Wales Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "log(% Disadvantaged Students)",
       y = "log(Average Attainment 8 Score)") +
  theme_minimal()
```

-   The log($Y$) log($X$) model is a standard transformation of these variables that can help to linearise relationships
-   Sometimes referred to as an **elasticity** model - a % (rather than constant) change in $X$ leads to a % (not constant) change in $Y$

## Linear Regression - the log-log transformation

```{r}
#| label: attainment-8-histogram4
#| fig.width: 8
#| fig.height: 5
#| fig.align: "center"

library(ggplot2)
library(dplyr)


# Calculate log-transformed summary stats
mean_value <- mean(log(england_filtered$ATT8SCR), na.rm = TRUE)
median_value <- median(log(england_filtered$ATT8SCR), na.rm = TRUE)
sd_value <- sd(log(england_filtered$ATT8SCR), na.rm = TRUE)

ggplot(england_filtered, aes(x = log(ATT8SCR))) +
  geom_histogram(aes(y = ..density..), binwidth = 0.05, fill = "#4E3C56", alpha = 0.4) +
  stat_function(fun = dnorm, args = list(mean = mean_value, sd = sd_value),
                color = "#2E6260", size = 1) +
  geom_vline(xintercept = median_value, colour = "black", linetype = "dotted", size = 1) +
  geom_vline(xintercept = mean_value, colour = "#F9DD73", linetype = "solid", size = 1) +
  annotate("text",
           x = median_value, y = Inf,
           label = paste0("Median = ", round(median_value, 2)),
           vjust = 1.3, colour = "black", size = 3.5) +
  annotate("text",
           x = mean_value, y = Inf,
           label = paste0("Mean = ", round(mean_value, 2)),
           vjust = 30.5, colour = "#F9DD73", size = 3.5) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.08))) +
  labs(
    title = "Log-Transformed Attainment 8 - Academy and Maintained Schools 2022/23",
    x = "Log(Attainment 8 Score)",
    y = "Density"
  ) +
  theme_minimal()
```

## Linear Regression - the log-log transformation

```{r}
mean_ptfsm <- mean(log(england_filtered$PTFSM6CLA1A), na.rm = TRUE)
median_ptfsm <- median(log(england_filtered$PTFSM6CLA1A), na.rm = TRUE)
sd_ptfsm <- sd(log(england_filtered$PTFSM6CLA1A), na.rm = TRUE)

ggplot(england_filtered, aes(x = log(PTFSM6CLA1A))) +
  geom_histogram(aes(y = ..density..), binwidth = 0.15, fill = "#4E3C56", alpha = 0.4) +
  stat_function(fun = dnorm, args = list(mean = mean_ptfsm, sd = sd_ptfsm),
                color = "#2E6260", size = 1) +
  geom_vline(xintercept = median_ptfsm, colour = "black", linetype = "dotted", size = 1) +
  geom_vline(xintercept = mean_ptfsm, colour = "#F9DD73", linetype = "solid", size = 1) +
  annotate("text",
           x = median_ptfsm, y = Inf,
           label = paste0("Median = ", round(median_ptfsm, 2)),
           vjust = 1.3, colour = "black", size = 3.5) +
  annotate("text",
           x = mean_ptfsm, y = Inf,
           label = paste0("Mean = ", round(mean_ptfsm, 2)),
           vjust = 30.5, colour = "#F9DD73", size = 3.5) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.08))) +
  labs(
    title = "Log-Transformed % Disadvantaged Students - Academy and Maintained Schools 2022/23",
    x = "Log(% Disadvantaged Students)",
    y = "Density"
  ) +
  theme_minimal()
```

## Linear Regression - the log-log transformation

```{r}
summary(lm_fit)
```

-   Everything now looks highly statistically significant
-   $R^2$ is much better than before - 45%

## Linear Regression - the log-log transformation

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("linearity"))
```

-   The residuals are now randomly scattered around zero - suggesting a linear relationship

## Linear Regression - the log-log transformation

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("qq"))
```

-   The residuals are now normally distributed - the Q-Q plot shows most points on the line

## Linear Regression - the log-log transformation

-   Interpreting the log-log (elasticity) relationship:
    -   Where the relationship is ***negative***, a small absolute change in the independent variable at the ***lower*** end has a ***bigger*** impact than the same small absolute change at the higher end
    -   On our original plot: the effect of levels of disadvantage on attainment is much ***stronger at very low levels of disadvantage*** and ***weakens as the level of disadvantage increases***

## Linear Regression - Level-log model

$$Y = \beta_0 + \beta_1log(X_1) + \epsilon$$

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 40%

england_filtered <- england_filtered %>%
  filter(PTFSM6CLA1A > 0, ATT8SCR > 0)
england_filtered <- england_filtered %>%
  mutate(log_att8 = log(ATT8SCR),
         log_ptfsm = log(PTFSM6CLA1A))

btn_sub <- btn_sub %>%
  mutate(log_att8 = log(ATT8SCR),
         log_ptfsm = log(PTFSM6CLA1A))

# Fit model
lm_fit <- lm(ATT8SCR ~ log(PTFSM6CLA1A), data = england_filtered)
lm_fit_a <- lm(log(ATT8SCR) ~ log(PTFSM6CLA1A), data = england_filtered)
lm_fit_b <- lm(log(ATT8SCR) ~ log(PERCTOT), data = england_filtered)

# Extract coefficients
slope <- round(coef(lm_fit)[2], 2)
intercept <- round(coef(lm_fit)[1], 2)

# Extract R²
r_squared <- round(summary(lm_fit)$r.squared, 2)

# Create annotation text
annotation_text <- paste0("y = ", intercept, " + ", slope, "x\nR² = ", r_squared)

# Plot with annotation
ggplot(england_filtered) +
  geom_point(aes(x = log_ptfsm, y = ATT8SCR, alpha = 0.6)) +  # Base layer: all schools
  geom_point(data = btn_sub, aes(x = log_ptfsm, y = ATT8SCR), 
             color = "#F9DD73", size = 2.5) +  # Overlay: Brighton & Hove schools
  geom_smooth(aes(x = log_ptfsm, y = ATT8SCR), method = "lm", 
              se = FALSE, color = "#8F5289") +
  annotate("text", x = max(england_filtered$log_ptfsm, na.rm = TRUE), 
           y = min(england_filtered$ATT8SCR, na.rm = TRUE), 
           label = annotation_text, hjust = 4, vjust = -2, size = 4, color = "#4E3C56") +
  labs(title = "England and Wales Schools - Attainment 8 vs % Disadvantaged Students, 2022-23",
       x = "log(% Disadvantaged Students)",
       y = "Average Attainment 8 Score") +
  theme_minimal()
```

-   Other options are to use a ***level-log*** model, where the dependent variable is not transformed but the independent variable is log-transformed.
-   Use when the effect of the independent variable has diminishing returns - e.g. a change from $log(2.7%)$ or $exp(1)$ to $log(7.3%)$ $exp(2)$ disadvantaged students has same effect as a change from $log(20%)$ $exp(3)$ to $log(54.5%)$ $exp(4)$

## Linear Regression - Level-log model

```{r}
summary(lm_fit)
```

## Linear Regression - Level-log model

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("linearity"))
```

## Linear Regression - Level-log model

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| out-width: 70%

library(performance)

check_model(lm_fit, 
            check = c("qq"))
```

## Linear Regression - to log or not to log?

-   The log-log model appears a better fit than the level-log model, however both are better than the basic linear model
-   How to proceed will depend a little on whether we want to extend our model with other explanatory variables and the relationship they have with attainment
-   Log-log - use when the relationship is believed to be multiplicative and the effects are best understood in terms of percentage changes.
    -   This model is also excellent at addressing issues with heteroscedasticity and skewed variables
-   Level-log - use when the effect of the independent variable has diminishing returns

## Conclusions

-   Regression Basics - DONE!
-   Won't summarise here, but please go over the slides again in your own time
-   Now is your chance to put everything I have just said into practice in your own very similar analysis in the practical
-   Practical is long - you won't complete in this session, but spend time going through it at home
-   Make sure you complete the practical and understand all of this before next week's Regression Sessions Vol. 2 as it will build on and extend this session

## Extension Activities

#### Extension 1 - Data Visualisation

